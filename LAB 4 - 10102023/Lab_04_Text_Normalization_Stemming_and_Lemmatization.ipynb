{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzGBvgsEmOuI"
   },
   "source": [
    "# Stem vs. Lemma vs. Lexeme\n",
    "#### A lemma is a word that stands at the head of a definition in a dictionary. All the head words in a dictionary are lemmas.\n",
    "#### A lexeme is a unit of meaning, and can be more than one word. A lexeme is the set of all forms that have the same meaning.\n",
    "#### In computational linguistics, a stem is the part of the word that never changes even when different forms of the word are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBeWR7aBmOuc"
   },
   "source": [
    "## Stemmers --> PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "55pxEGo6mOuf",
    "outputId": "f7e1af4d-2f6c-48f9-983c-b90784e0962a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "list\n",
      "list\n",
      "list\n",
      "list\n",
      "list\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words1 = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "example_words2 = [\"List\", \"listed\", \"lists\", \"listing\", \"listings\"]\n",
    "\n",
    "for w in example_words1:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "for w in example_words2:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xPiZEv0OmOuo",
    "outputId": "4867609e-986d-4477-b91c-e6ca19796a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'veri', 'import', 'to', 'be', 'pythonli', 'while', 'you', 'are', 'python', 'with', 'python', '.', 'all', 'python', 'have', 'python', 'poorli', 'at', 'least', 'onc', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "new_text = \"\"\"It is very important to be pythonly while you are pythoning\n",
    "        with python. All pythoners have pythoned poorly at least once.\"\"\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "print([ps.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRHJmSHZmOur"
   },
   "source": [
    "## Stemmers --> LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WtAlJtUXmOus",
    "outputId": "21ac221b-a76a-4172-d680-d6256238a00b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "\n",
      "\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "    is no basis for a system of government.  Supreme executive power derives from\n",
    "    a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print(\"\\n\")\n",
    "print([lancaster.stem(t) for t in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW7h3s9mmOuv"
   },
   "source": [
    "## Lemmatization --> WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZJfV0xsmOux",
    "outputId": "452c2bb2-76f6-4ebc-9510-46460dec9326"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer, PorterStemmer\n\u001b[1;32m----> 2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrocks :\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrocks\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"\\nproduced :\", lemmatizer.lemmatize(\"produced\", pos = 'v'))\n",
    "\n",
    "ps = PorterStemmer()\n",
    "print(\"\\nStem of the word produced :\", ps.stem(\"produced\"))\n",
    "\n",
    "print(\"\\nbetter :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "\n",
    "print(\"\\nwomen :\", lemmatizer.lemmatize(\"women\", pos =\"n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1jNoCT9mOu2",
    "outputId": "03191ed8-9202-4c8f-bc4d-cb1ce985a5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "\n",
      "DENNIS              DENNIS              \n",
      ":                   :                   \n",
      "Listen              Listen              \n",
      ",                   ,                   \n",
      "strange             strange             \n",
      "women               women               \n",
      "lying               lie                 \n",
      "in                  in                  \n",
      "ponds               ponds               \n",
      "distributing        distribute          \n",
      "swords              swords              \n",
      "is                  be                  \n",
      "no                  no                  \n",
      "basis               basis               \n",
      "for                 for                 \n",
      "a                   a                   \n",
      "system              system              \n",
      "of                  of                  \n",
      "government          government          \n",
      ".                   .                   \n",
      "Supreme             Supreme             \n",
      "executive           executive           \n",
      "power               power               \n",
      "derives             derive              \n",
      "from                from                \n",
      "a                   a                   \n",
      "mandate             mandate             \n",
      "from                from                \n",
      "the                 the                 \n",
      "masses              mass                \n",
      ",                   ,                   \n",
      "not                 not                 \n",
      "from                from                \n",
      "some                some                \n",
      "farcical            farcical            \n",
      "aquatic             aquatic             \n",
      "ceremony            ceremony            \n",
      ".                   .                   \n",
      "\n",
      "['List', 'listed', 'list', 'listing', 'listing']\n",
      "\n",
      "List                List                \n",
      "listed              list                \n",
      "lists               list                \n",
      "listing             list                \n",
      "listings            list                \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "    is no basis for a system of government.  Supreme executive power derives from\n",
    "    a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])\n",
    "print()\n",
    "\n",
    "for t in tokens:\n",
    "    print (\"{0:20}{1:20}\".format(t, wnl.lemmatize(t, pos=\"v\")))\n",
    "print()\n",
    "example_words = [\"List\", \"listed\", \"lists\", \"listing\", \"listings\"]\n",
    "print([wnl.lemmatize(w) for w in example_words])\n",
    "print()\n",
    "for words in example_words:\n",
    "    print (\"{0:20}{1:20}\".format(words, wnl.lemmatize(words, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DPjJ4YZmOu4"
   },
   "source": [
    "# Lemmatization using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d47Ou3jwmOu5",
    "outputId": "249695d3-3ec2-4ba7-8d68-00945c2e3660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\anaconda\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\anaconda\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "['DENNIS', 'Listen', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony']\n",
      "\n",
      "['DENNIS', 'Listen', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony']\n",
      "\n",
      "DENNIS              DENNIS              \n",
      "Listen              Listen              \n",
      "strange             strange             \n",
      "women               women               \n",
      "lying               lie                 \n",
      "in                  in                  \n",
      "ponds               ponds               \n",
      "distributing        distribute          \n",
      "swords              swords              \n",
      "is                  be                  \n",
      "no                  no                  \n",
      "basis               basis               \n",
      "for                 for                 \n",
      "a                   a                   \n",
      "system              system              \n",
      "of                  of                  \n",
      "government          government          \n",
      "Supreme             Supreme             \n",
      "executive           executive           \n",
      "power               power               \n",
      "derives             derive              \n",
      "from                from                \n",
      "a                   a                   \n",
      "mandate             mandate             \n",
      "from                from                \n",
      "the                 the                 \n",
      "masses              mass                \n",
      "not                 not                 \n",
      "from                from                \n",
      "some                some                \n",
      "farcical            farcical            \n",
      "aquatic             aquatic             \n",
      "ceremony            ceremony            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Faithlin\n",
      "[nltk_data]     Hoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sentence = TextBlob('DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.')\n",
    "tokens = sentence.words\n",
    "print(tokens)\n",
    "print()\n",
    "print(tokens.lemmatize())\n",
    "print()\n",
    "\n",
    "for t in tokens:\n",
    "    print (\"{0:20}{1:20}\".format(t, wnl.lemmatize(t, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BlC0XFXmOu6",
    "outputId": "139811b8-7061-47ce-bb2b-e1957319a62a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['List', 'listed', 'lists', 'listing', 'listings']\n",
      "\n",
      " ['List', 'listed', 'list', 'listing', 'listing']\n",
      "\n",
      "List                List                \n",
      "listed              list                \n",
      "lists               list                \n",
      "listing             list                \n",
      "listings            list                \n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "text = TextBlob(\"List listed lists listing listings\")\n",
    "tokens = text.words\n",
    "print(tokens)\n",
    "print(\"\\n\",tokens.lemmatize())\n",
    "print()\n",
    "for t in tokens:\n",
    "    print (\"{0:20}{1:20}\".format(t, wnl.lemmatize(t, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsQ5MIQ812bz"
   },
   "source": [
    "# Lemmatization using SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD7NJRsR12mK"
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "    is no basis for a system of government.  Supreme executive power derives from\n",
    "    a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "doc = nlp(raw)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token} -> {token.lemma_}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWjDRkRPmOu7"
   },
   "source": [
    "# Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlpTiNJOmOu8"
   },
   "source": [
    "### Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odGmHicbmOu8",
    "outputId": "82418e56-68b5-44cd-834c-07ad3f451fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dennis: listen, strange women lying in ponds distributing swords\n",
      "    is no basis for a system of government.  supreme executive power derives from\n",
      "    a mandate from the masses, not from some farcical aquatic ceremony.\n",
      "\n",
      "Tokens\n",
      "['dennis', ':', 'listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "\n",
      "Lemmas\n",
      "['dennis', ':', 'listen', ',', 'strange', 'women', 'lie', 'in', 'ponds', 'distribute', 'swords', 'be', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'supreme', 'executive', 'power', 'derive', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "\n",
      "Porter Stemming\n",
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "\n",
      "Lancaster Stemming\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n",
      "\n",
      "Snowball Stemming\n",
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#file = open (\"D:/APU/TXSA-CT107-3-3/TUTORIAL/sample01.txt\")\n",
    "#raw = file.read()\n",
    "\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "    is no basis for a system of government.  Supreme executive power derives from\n",
    "    a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "words = raw.lower()\n",
    "print(words)\n",
    "print()\n",
    "tokens = word_tokenize(words)\n",
    "print(\"Tokens\")\n",
    "print(tokens)\n",
    "print()\n",
    "print(\"Lemmas\")\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t, pos = \"v\") for t in tokens])\n",
    "print()\n",
    "print(\"Porter Stemming\")\n",
    "ps = PorterStemmer()\n",
    "print ([ps.stem(t) for t in tokens])\n",
    "print()\n",
    "print(\"Lancaster Stemming\")\n",
    "ls = LancasterStemmer()\n",
    "print ([ls.stem(t) for t in tokens])\n",
    "print()\n",
    "print(\"Snowball Stemming\")\n",
    "sn = nltk.SnowballStemmer(\"english\")\n",
    "print([sn.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ATI6roEmOu_"
   },
   "source": [
    "# Stemmers --> SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amzF9MVCmOvG",
    "outputId": "22ef7dc3-36b6-47e4-ef6d-6dcdc2c2758c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "16\n",
      "\n",
      "\n",
      "['this', 'is', 'achiev', 'in', 'practic', 'dure', 'stem', ',', 'a', 'text', 'preprocess', 'oper', '.']\n",
      "\n",
      "\n",
      "['cec', 'est', 'réalis', 'en', 'pratiqu', 'lor', 'du', 'stemming', ',', 'une', 'oper', 'de', 'prétrait', 'de', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(nltk.SnowballStemmer.languages)\n",
    "print(len(nltk.SnowballStemmer.languages))\n",
    "print()\n",
    "text = \"This is achieved in practice during stemming, a text preprocessing operation.\"\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print()\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "print([stemmer.stem(t) for t in tokens])\n",
    "print()\n",
    "text2 = \"Ceci est réalisé en pratique lors du stemming, une opération de prétraitement de texte.\"\n",
    "tokens2 = nltk.tokenize.word_tokenize(text2)\n",
    "print()\n",
    "stemmer = nltk.SnowballStemmer('french')\n",
    "print([stemmer.stem(t) for t in tokens2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwF0qc5KmOvI"
   },
   "source": [
    "## SnowballStemmer --> for other space delimited languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQZcUDcpTLJK"
   },
   "source": [
    "Lets see how to **detect and translate a language**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq1JL2t-TcAF"
   },
   "source": [
    "**langdetect** supports 55 languages out of the box (ISO 639-1 codes)\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FM9LhgEPG1F",
    "outputId": "46fa0a1a-6c01-4059-e479-f3ac6d32de2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 18.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=987f8ac1e7ab9bf5ccea3ce8c45896aaa3472ec82de1ff005309c460a5953d5c\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSvdkYi6mOvJ",
    "outputId": "ccf6fa3a-5639-4b38-a643-529d83576ffe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Ceci est réalisé dans la pratique pendant la tige, une opération de prétraitement du texte.\n",
      "['Ceci', 'est', 'réalisé', 'dans', 'la', 'pratique', 'pendant', 'la', 'tige', 'une', 'opération', 'de', 'prétraitement', 'du', 'texte']\n",
      "\n",
      "['cec', 'est', 'réalis', 'dan', 'la', 'pratiqu', 'pend', 'la', 'tig', 'une', 'oper', 'de', 'prétrait', 'du', 'text']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from langdetect import detect\n",
    "text = \"This is achieved in practice during stemming, a text preprocessing operation.\"\n",
    "print(detect(text))\n",
    "\n",
    "en_blob = TextBlob(text)\n",
    "fr_blob = en_blob.translate(from_lang=\"en\", to='fr')\n",
    "print(fr_blob)\n",
    "\n",
    "tokens = fr_blob.words\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "stemmer = nltk.SnowballStemmer('french')\n",
    "print([stemmer.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9rOvYNiTB6L",
    "outputId": "87f3e92b-aaf3-4663-dd9d-c87bb38c25da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting translate==3.6.1\n",
      "  Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libretranslatepy==2.1.1\n",
      "  Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from translate==3.6.1) (2.23.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from translate==3.6.1) (7.1.2)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from translate==3.6.1) (4.9.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->translate==3.6.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->translate==3.6.1) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->translate==3.6.1) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->translate==3.6.1) (3.0.4)\n",
      "Installing collected packages: libretranslatepy, translate\n",
      "Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n"
     ]
    }
   ],
   "source": [
    "# Language Detection and Translation\n",
    "!pip install translate==3.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXnBvE_dS5gW",
    "outputId": "42263923-c16e-41a4-df16-6ef0a1a38ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "This is done in practice during stemming, a text preprocessing operation\n"
     ]
    }
   ],
   "source": [
    "# Language Detection and Translation\n",
    "from langdetect import detect\n",
    "text = \"Ceci est réalisé en pratique lors du stemming, une opération de prétraitement de texte\"\n",
    "print(detect(text))\n",
    "\n",
    "from translate import Translator\n",
    "translator = Translator(from_lang = 'fr', to_lang='en')\n",
    "print(translator.translate(text))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
