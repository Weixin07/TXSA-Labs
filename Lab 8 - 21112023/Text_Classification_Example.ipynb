{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqcLIK_NCcvI",
    "outputId": "2269fd06-0397-4c68-c040-b3c8de5e378a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test document is classified as:  j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Faithlin\n",
      "[nltk_data]     Hoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "def extract_words(words):\n",
    "  return dict([(word, True) for word in words])\n",
    "\n",
    "m_data = ['TPM Tianjin TPM', 'TPM TPM Shanghai', 'TPM Macao']\n",
    "j_data = ['Chiba Kobe Kyoto TPM', 'Chiba Kobe Macao']\n",
    "\n",
    "m_feats = [(extract_words(nltk.tokenize.word_tokenize(m_data[i[0]])), 'm') for i in enumerate(m_data)]\n",
    "j_feats = [(extract_words(nltk.tokenize.word_tokenize(j_data[i[0]])), 'j') for i in enumerate(j_data)]\n",
    "\n",
    "train_data = m_feats + j_feats\n",
    "\n",
    "clf = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print('The test document is classified as: ', clf.classify(extract_words(nltk.tokenize.word_tokenize('TPM Kyoto Macao Chiba Kobe'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvo4B909pMQa"
   },
   "source": [
    "**Testing word not found situation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMF3WH6uXjwL",
    "outputId": "db08b8a6-5516-40c1-91cf-47a7c651be0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "The test document is classified as:  m\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "def extract_words(words):\n",
    "  return dict([(word, True) for word in words])\n",
    "\n",
    "m_data = ['TPM Tianjin TPM', 'TPM TPM Shanghai', 'TPM Macao']\n",
    "j_data = ['Chiba Kobe Kyoto TPM', 'Chiba Kobe Macao']\n",
    "\n",
    "m_feats = [(extract_words(nltk.tokenize.word_tokenize(m_data[i[0]])), 'm') for i in enumerate(m_data)]\n",
    "j_feats = [(extract_words(nltk.tokenize.word_tokenize(j_data[i[0]])), 'j') for i in enumerate(j_data)]\n",
    "\n",
    "train_data = m_feats + j_feats\n",
    "\n",
    "clf = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print('The test document is classified as: ', clf.classify(extract_words(nltk.tokenize.word_tokenize('india'))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification - Example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
